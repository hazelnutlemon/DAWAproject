{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표본조사를 위한 날짜를 랜덤으로 선택합니다.\n",
      "선택된 날짜 :  [21, 24, 29, 19, 23, 25, 12, 6, 8, 17, 22, 13, 2, 26, 28]\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "약빨이 신선함에 대한 작품 분석을 위한 크롤링을 시작합니다.\n",
      "- 약빨이 신선함에 대한 Dcinside 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Dcinside 크롤링이 완료되었습니다.\n",
      "- 약빨이 신선함에 대한 Naver Blog 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Naver Blog 크롤링이 완료되었습니다.\n",
      "- 약빨이 신선함에 대한 Daum Blog 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Daum Blog 크롤링이 완료되었습니다.\n",
      "- 약빨이 신선함에 대한 Tistory Blog 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Tistory Blog 크롤링이 완료되었습니다.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "약빨이 신선함에 대한 TextRank 분석을 시작합니다.\n",
      "- 약빨이 신선함에 대한 TextRank 분석결과 \n",
      "['한 ', '유화 부인 ', '금 손 ', '담 덕 ', '동해 용왕 ']\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "약빨이 신선함에 대한 Aspect Mining 분석을 시작합니다.\n",
      "- 약빨이 신선함에 대한 Aspect Mining 분석결과 (주인공, 스토리, 분위기 키워드순)\n",
      "['성장하다', '평범하다', '다르다', '뛰어나다', '미미하다']\n",
      "['성장하다', '미미하다', '재밌다', '생소하다', '평범하다']\n",
      "['뻔하다', '거대하다', '험하다', '정당하다', '어리둥절하다']\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "약빨이 신선함에 대한 Sentimental Analysis 분석을 시작합니다.\n",
      "- 약빨이 신선함에 대한 Joara의 독자 리뷰 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Joara의 독자 리뷰 크롤링이 완료되었습니다.\n",
      "- 약빨이 신선함에 대한 Ridibooks의 독자 리뷰 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Ridibooks의 독자 리뷰 크롤링이 완료되었습니다.\n",
      "- 약빨이 신선함에 대한 Naver Series의 독자 리뷰 크롤링을 진행하고 있습니다.\n",
      "- 약빨이 신선함에 대한 Naver Serieson의 독자 리뷰 크롤링이 완료되었습니다.\n",
      "\n",
      "\n",
      "- 약빨이 신선함에 대한 Sentimental 분석결과 ( 2020.01 ~ 2020.10 ) \n",
      "['41.66', '51.96', '46.93', '47.84', '52.75', '49.91', '37.94', '48.69', '47.36', '46.20']\n",
      "\n",
      "\n",
      "약빨이 신선함에 대한 작품 분석을 위한 크롤링이 모두 완료되었습니다.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "카드빨로 축구 스타에 대한 작품 분석을 위한 크롤링을 시작합니다.\n",
      "- 카드빨로 축구 스타에 대한 Dcinside 크롤링을 진행하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "#단, best_model.h5, test_stage_1, train_stage_1은 현재 실행하고 있는 폴더 내에 복사해서 붙여넣기 필수!\n",
    "#층화 추출법\n",
    "# DB받아야하고\n",
    "# 개인별로 몇개할지 정하고\n",
    "# 코드수정할 부분 파악하고\n",
    "# 텍스트 파일이 2개씩 생성되니까 그거 파악해야하고\n",
    "# 주말까진 해야하고\n",
    "import re\n",
    "import Sentimental_Analyzer as SA\n",
    "import Date_List as DL\n",
    "import BSC_Crawler as crawler\n",
    "from Hot_Topic_Analyzer import hot_topic_analyzer\n",
    "from Aspect_Miner import aspect_miner\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import random\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "# 무작위 10일을 선택.\n",
    "select_date = random.sample(range(1,30),15) # 1부터 29 중에 10개를 중복없이 뽑는다. -> type : list\n",
    "print(\"표본조사를 위한 날짜를 랜덤으로 선택합니다.\")\n",
    "print(\"선택된 날짜 : \", select_date)\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/chromedriver.exe\")\n",
    "\n",
    "conn=pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=3306,\n",
    "    user='root',\n",
    "    passwd='3721',\n",
    "    db='webnoveldb',\n",
    "    charset='utf8'\n",
    ")\n",
    "\n",
    "curs=conn.cursor()\n",
    "sql=\"select * from novellink where (idnovel >95 ) and (idnovel <= 500)\"\n",
    "#####################################id novel 1~99, 501~693, 5001~5031 완료########################################\n",
    "\n",
    "\n",
    "curs.execute(sql)\n",
    "\n",
    "rows=curs.fetchall()\n",
    "\n",
    "novel_lists =[]\n",
    "for row in rows:\n",
    "    novel_lists.append(list(row)) #'작품ID', '작품제목', '조아라','문피아','카카오페이지','리디북스','시리즈온','네이버 웹소설']\n",
    "\n",
    "#crawler.ig_login(driver)\n",
    "crawler.ridi_login(driver)\n",
    "crawler.naver_login(driver)\n",
    "\n",
    "for novel_list in novel_lists:\n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    work_name=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', novel_list[1]).replace(\"  \",\" \")\n",
    "    file_name='Crawling\\\\' + work_name +'.txt'\n",
    "    print(work_name + \"에 대한 작품 분석을 위한 크롤링을 시작합니다.\")\n",
    "    ####################인스타그램 크롤러################\n",
    "    #instagram_list=[]\n",
    "    #print(work_name + \"에 대한 Instagram 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "    #instagram_list=crawler.ig_scraping(driver,novel_list[1])\n",
    "    #print(work_name + \"에 대한 Naver Blog 크롤링이 완료되었습니다.\")\n",
    "\n",
    "    #data = pd.DataFrame(instagram_list, columns = ['내용', '작성 날짜'])\n",
    "    #data\n",
    "\n",
    "    ####################디시인사이드 크롤러##################\n",
    "    dcinside_list=[]\n",
    "    try:\n",
    "        print(\"- \"+ work_name + \"에 대한 Dcinside 크롤링을 진행하고 있습니다.\")\n",
    "        dcinside_list=crawler.dc_scraping(driver, novel_list[1])\n",
    "        print(\"- \"+ work_name + \"에 대한 Dcinside 크롤링이 완료되었습니다.\")\n",
    "        #df = pd.DataFrame(dcinside_list, columns = ['작성 날짜', '제목', '내용 + 댓글'])\n",
    "        #df\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ####################인스티즈 크롤러##############\n",
    "    #instiz_list_list=[]\n",
    "    #print(work_name + \"에 대한 Instiz 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "    #instiz_list=crawler.dc_scraping(driver, novel_list[985][1])\n",
    "    #print(work_name + \"에 대한 Naver Blog 크롤링이 완료되었습니다.\")\n",
    "\n",
    "\n",
    "    ####################네이버 크롤러################\n",
    "    naver_list=[]\n",
    "    print(\"- \"+ work_name + \"에 대한 Naver Blog 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "    naver_list=crawler.naver_crawling(novel_list[1])\n",
    "    print(\"- \"+ work_name + \"에 대한 Naver Blog 크롤링이 완료되었습니다.\")\n",
    "    ####################다음 크롤러##################\n",
    "    daum_list=[]\n",
    "    print(\"- \"+ work_name + \"에 대한 Daum Blog 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "    try:\n",
    "        daum_list=crawler.daum_crawling(novel_list[1])\n",
    "        print(\"- \"+ work_name + \"에 대한 Daum Blog 크롤링이 완료되었습니다.\")\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    ####################티스토리 크롤러##############\n",
    "    tistory_list=[]\n",
    "    try:\n",
    "        print(\"- \"+ work_name + \"에 대한 Tistory Blog 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "        tistory_list=crawler.tistory_crawling(novel_list[1])\n",
    "        print(\"- \"+ work_name + \"에 대한 Tistory Blog 크롤링이 완료되었습니다.\")\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    #[title, author, date, post]\n",
    "\n",
    "    ############크롤러 결과중 내용부분만 textfile화############\n",
    "\n",
    "    f=open(file_name, 'a',encoding=\"utf-8\")\n",
    "    for content in dcinside_list:\n",
    "        content[2]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[2])\n",
    "        f.write(\"%s\\n\" % content[2])\n",
    "    for content in dcinside_list:\n",
    "        content[2]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[2])\n",
    "        f.write(\"%s\\n\" % content[2])\n",
    "    for content in naver_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    for content in daum_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    for content in daum_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    for content in naver_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    for content in daum_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    for content in daum_list:\n",
    "        content[3]=re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,]', '', content[3])\n",
    "        f.write(\"%s\\n\" % content[3])\n",
    "    f.close()\n",
    "    #f=open('Crawling\\\\{}.txt'.format(re.sub('[^ ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\.\\,], '', novel_list[1])), 'a',encoding=\"utf-8\")\n",
    "    #with open('{}TR.txt'.format(title), 'a',encoding=\"utf-8\") as f:\n",
    "    #for content in instagram_list:\n",
    "    #    content[0]=re.sub('[^ ㄱ-ㅎ|ㅏ-ㅣ|가-힣a-zA-Z0-9.]', '', content[1])\n",
    "    #    f.write(\"%s\\n\" % content[1])\n",
    "\n",
    "    #for content in instiz_list:\n",
    "    #    content[3]=re.sub('[^ ㄱ-ㅎ|ㅏ-ㅣ|가-힣a-zA-Z0-9\\:\\/\\-\\+\\*\\~\\!\\`\\'\\\"\\!\\@\\#\\$\\%\\^\\&\\*\\(\\.\\,\\<\\/\\?\\:\\;\\[\\]\\{\\}\\\\\\|]', '', content[3])\n",
    "    #    f.write(\"%s\\n\" % content[3])\n",
    "    \n",
    "    ############Textrank 분석############\n",
    "    topic_list=[]\n",
    "    print('\\n')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    print(work_name + \"에 대한 TextRank 분석을 시작합니다.\")\n",
    "    topic_list=hot_topic_analyzer(work_name)\n",
    "    if(topic_list==None):\n",
    "        topic_list=[]\n",
    "        for i in range(0,5):\n",
    "            topic_list.append('NULL')\n",
    "    print(\"- \"+ work_name + \"에 대한 TextRank 분석결과 \")\n",
    "    print(topic_list)\n",
    "    \n",
    "    \n",
    "    ###########Aspect mining 분석########\\\n",
    "    character_word_list=[]\n",
    "    story_word_list=[]\n",
    "    mood_word_list=[]\n",
    "    print('\\n')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    print(work_name + \"에 대한 Aspect Mining 분석을 시작합니다.\")\n",
    "    character_word_list, story_word_list, mood_word_list = aspect_miner(work_name)\n",
    "    print(\"- \"+ work_name + \"에 대한 Aspect Mining 분석결과 (주인공, 스토리, 분위기 키워드순)\")\n",
    "    print(character_word_list)\n",
    "    print(story_word_list)\n",
    "    print(mood_word_list)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        character_word_list, story_word_list, mood_word_list = aspect_miner(novel_list[1])\n",
    "\n",
    "    except:\n",
    "        for i in range(0,5):\n",
    "            character_word_list.append('NULL')\n",
    "            story_word_list.append('NULL')\n",
    "            mood_word_list.append('NULL')\n",
    "    \"\"\"\n",
    "    f_w = open('TA_analysis_result.csv', 'a', encoding='cp949', newline='')\n",
    "    wr = csv.writer(f_w)\n",
    "    result=topic_list+character_word_list+story_word_list+mood_word_list\n",
    "    result.insert(0,novel_list[0])\n",
    "    wr.writerow(result)\n",
    "    f_w.close()\n",
    "\n",
    "    \n",
    "    #########################################################감성분석#########################################################\n",
    "\n",
    "    date_list=DL.make_date_list()\n",
    "    print('\\n')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    print(work_name + \"에 대한 Sentimental Analysis 분석을 시작합니다.\")\n",
    "    \n",
    "    if(novel_list[2]):\n",
    "        print(\"- \"+ work_name + \"에 대한 Joara의 독자 리뷰 크롤링을 진행하고 있습니다.\")\n",
    "        joara_comments=crawler.joara_comments(driver,novel_list[2])\n",
    "        print(\"- \"+ work_name + \"에 대한 Joara의 독자 리뷰 크롤링이 완료되었습니다.\")\n",
    "\n",
    "        #data = pd.DataFrame(joara_comments, columns = ['내용', '작성 날짜'])\n",
    "        #print(data)\n",
    "        for review in joara_comments:\n",
    "            score=float(SA.predict(review[1]))\n",
    "            pos=DL.date_list_pos(review[0])\n",
    "            date_list[pos][1]=date_list[pos][1] + score\n",
    "            date_list[pos][2]+=1\n",
    "\n",
    "    if(novel_list[5]):\n",
    "        print(\"- \"+ work_name + \"에 대한 Ridibooks의 독자 리뷰 크롤링을 진행하고 있습니다.\")\n",
    "        ridi_comments=crawler.ridi_comments(select_date,driver,novel_list[5])\n",
    "        print(\"- \"+ work_name + \"에 대한 Ridibooks의 독자 리뷰 크롤링이 완료되었습니다.\")\n",
    "\n",
    "        #data = pd.DataFrame(ridi_comments, columns = ['내용', '작성 날짜'])\n",
    "        #print(data)\n",
    "        for review in ridi_comments:\n",
    "            score=float(SA.predict(review[1]))\n",
    "            pos=DL.date_list_pos(review[0])\n",
    "            date_list[pos][1]=date_list[pos][1] + score\n",
    "            date_list[pos][2]+=1\n",
    "\n",
    "    if(novel_list[6]):\n",
    "        print(\"- \"+ work_name + \"에 대한 Naver Series의 독자 리뷰 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "        serieson_comments=crawler.serieson_comments(select_date,driver,novel_list[6])\n",
    "        print(\"- \"+ work_name + \"에 대한 Naver Serieson의 독자 리뷰 크롤링이 완료되었습니다.\")\n",
    "\n",
    "        #data = pd.DataFrame(serieson_comments, columns = ['내용', '작성 날짜'])\n",
    "        #data\n",
    "        for review in serieson_comments:\n",
    "            score=float(SA.predict(review[1]))\n",
    "            pos=DL.date_list_pos(review[0])\n",
    "            date_list[pos][1]=date_list[pos][1] + score\n",
    "            date_list[pos][2]+=1\n",
    "    \n",
    "    if(novel_list[7]):\n",
    "        print(\"- \"+ work_name + \"에 대한 Naver 웹소설의 독자 리뷰 크롤링을 진행하고 있습니다.\")\n",
    "\n",
    "        naver_comments=crawler.naver_comments(driver,novel_list[7])\n",
    "        print(\"- \"+ work_name + \"에 대한 Naver 웹소설의 독자 리뷰 크롤링이 완료되었습니다.\")\n",
    "\n",
    "        #data = pd.DataFrame(naver_comments, columns = ['내용', '작성 날짜'])\n",
    "        #print(data) <-> data\n",
    "        for review in naver_comments:\n",
    "            score=float(SA.predict(review[1]))\n",
    "            pos=DL.date_list_pos(review[0])\n",
    "            date_list[pos][1]=date_list[pos][1] + score\n",
    "            date_list[pos][2]+=1\n",
    "    \n",
    "\n",
    "\n",
    "    f_w = open('S_analysis_result.csv', 'a', encoding='cp949', newline='')\n",
    "    wr = csv.writer(f_w)\n",
    "    result4showing=[]\n",
    "    for date in date_list:\n",
    "        result=[]\n",
    "        result.append(date[0])\n",
    "        try:\n",
    "            result.append(\"%.2f\" % (date[1]/date[2]))\n",
    "            result4showing.append(\"%.2f\" % (date[1]/date[2]))\n",
    "        except:\n",
    "            result.append('0')\n",
    "            result4showing.append('0')\n",
    "            \n",
    "        result.insert(0,novel_list[0])\n",
    "        wr.writerow(result)\n",
    "    f_w.close()\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"- \"+ work_name + \"에 대한 Sentimental 분석결과 ( 2020.01 ~ 2020.10 ) \")\n",
    "    print(result4showing)\n",
    "    print('\\n')\n",
    "    print(work_name + \"에 대한 작품 분석을 위한 크롤링이 모두 완료되었습니다.\")\n",
    "    print('\\n')\n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
