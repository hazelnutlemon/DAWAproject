{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://bab2min.tistory.com/552\n",
    "import networkx\n",
    "import re\n",
    " \n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            #print(k[0], l[0], pairness[k, l])\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "        for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "응급 의료 \n",
      "동성 의료원 \n",
      "의사 \n",
      "한국 병원 \n",
      "다음 날 \n"
     ]
    }
   ],
   "source": [
    "tr = TextRank(window=5, coef=1)\n",
    "print('Load...')\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') ])\n",
    "tr.load(RawTaggerReader('닥터최태수.txt'), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP', 'VV', 'VA')))\n",
    "print('Build...')\n",
    "tr.build()\n",
    "kw = tr.extract(0.1)\n",
    "title=\"닥터최태수\"\n",
    "title_nonspace=title.replace(' ','')\n",
    "count=0\n",
    "for k in sorted(kw, key=kw.get, reverse=True):\n",
    "    temp=(\"%s%g\" % (k, kw[k])).split('0')[0]\n",
    "    if 'VV'in temp:\n",
    "        continue\n",
    "    if 'VA'in temp:\n",
    "        continue\n",
    "        \n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('', temp).replace('  ',' ') # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "    result_nonspace=result.replace(' ','')\n",
    "    if len(result)<=2:\n",
    "        continue\n",
    "    if result in title:\n",
    "        continue\n",
    "    if result_nonspace in title:\n",
    "        continue\n",
    "    if result in title_nonspace:\n",
    "        continue\n",
    "    if result_nonspace in title_nonspace:\n",
    "        continue\n",
    "    if '리뷰' in result:\n",
    "        continue\n",
    "    if '소설' in result:\n",
    "        continue\n",
    "    if '소개' in result:\n",
    "        continue\n",
    "    print (result)\n",
    "    count+=1\n",
    "    if(count==5):\n",
    "        break\n",
    "    #hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    #result = hangul.sub('', temp) # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "    #print (result)\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #print(\"%s%g\" % (k, kw[k]))\n",
    "    #print(\"%s\\t%g\" % (k, kw[k]))\n",
    "    #% (k, kw[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "(('닥터', 'NNP'), ('최태수', 'NNP'))\t0.140619\n",
      "(('태수', 'NNP'),)\t0.0827759\n",
      "(('응급', 'NNP'), ('의료', 'NNP'))\t0.0812925\n",
      "(('의학', 'NNP'), ('소설', 'NNG'))\t0.0750026\n",
      "(('동성', 'NNG'), ('의료원', 'NNG'))\t0.058908\n",
      "(('한국', 'NNP'), ('의사', 'NNG'))\t0.0462055\n",
      "(('다음', 'NNG'), ('날', 'NNG'))\t0.044866\n",
      "(('석호', 'NNP'), ('작가', 'NNG'))\t0.0410713\n",
      "(('소개', 'NNG'), ('글', 'NNG'))\t0.0399076\n",
      "(('인턴', 'NNP'), ('레지던트', 'NNP'))\t0.0395852\n",
      "(('이', 'NNP'), ('세계', 'NNG'))\t0.03857\n",
      "(('간호사', 'NNP'), ('카슈미르', 'NNP'))\t0.0374284\n",
      "(('곡리', 'NNP'), ('사람', 'NNG'))\t0.0360313\n",
      "(('해외', 'NNG'),)\t0.0347588\n",
      "(('지식', 'NNG'),)\t0.0342711\n",
      "(('하차', 'NNG'), ('작품', 'NNG'))\t0.0340872\n",
      "(('병원', 'NNG'),)\t0.0314249\n",
      "(('제임스', 'NNP'),)\t0.0312016\n",
      "(('때', 'NNG'), ('카프레', 'NNP'))\t0.0311674\n",
      "(('인물', 'NNG'),)\t0.0287055\n",
      "(('환자', 'NNG'),)\t0.0269754\n",
      "(('반복', 'NNG'),)\t0.0262148\n",
      "(('책', 'NNG'),)\t0.025769\n",
      "(('주인공', 'NNG'),)\t0.0252469\n",
      "(('후', 'NNG'),)\t0.0249263\n",
      "(('게', 'NNG'),)\t0.0232284\n",
      "(('이야기', 'NNG'),)\t0.0230664\n",
      "(('권위', 'NNG'),)\t0.0230172\n",
      "(('웹', 'NNG'),)\t0.0223112\n",
      "(('상황', 'NNG'),)\t0.0220984\n",
      "(('실력', 'NNG'),)\t0.0208609\n",
      "(('수술', 'NNG'),)\t0.0208491\n",
      "(('팀원', 'NNG'),)\t0.0208325\n",
      "(('만능', 'NNG'),)\t0.0205324\n",
      "(('과장', 'NNG'),)\t0.020513\n",
      "(('드라마', 'NNG'),)\t0.0204145\n",
      "(('감동', 'NNG'),)\t0.019868\n",
      "(('내용', 'NNG'),)\t0.0197526\n",
      "(('치료', 'NNG'),)\t0.0195901\n",
      "(('주인공', 'NNP'),)\t0.0193431\n",
      "(('판타지 소설', 'NNP'),)\t0.0182836\n",
      "(('죽음', 'NNG'),)\t0.01716\n",
      "(('생각', 'NNG'),)\t0.015076\n"
     ]
    }
   ],
   "source": [
    "tr = TextRank(window=5, coef=1)\n",
    "print('Load...')\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') ])\n",
    "tr.load(RawTaggerReader('닥터최태수.txt'), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP')))\n",
    "print('Build...')\n",
    "tr.build()\n",
    "kw = tr.extract(0.1)\n",
    "for k in sorted(kw, key=kw.get, reverse=True):\n",
    "    print(\"%s\\t%g\" % (k, kw[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
