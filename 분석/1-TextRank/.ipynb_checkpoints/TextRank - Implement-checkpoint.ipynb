{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://bab2min.tistory.com/552\n",
    "import networkx\n",
    "import re\n",
    " \n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='cp949'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='cp949'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            #print(k[0], l[0], pairness[k, l])\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "        for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "라온 \n",
      "윤 이수 \n",
      "드라마 \n",
      "병 연 \n",
      "화초 저하 \n"
     ]
    }
   ],
   "source": [
    "tr = TextRank(window=5, coef=1)\n",
    "print('Load...')\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') ])\n",
    "tr.load(RawTaggerReader('piece.txt'), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP', 'VV', 'VA')))\n",
    "print('Build...')\n",
    "tr.build()\n",
    "kw = tr.extract(0.1)\n",
    "title=\"구르미 그린 달빛\"\n",
    "title_nonspace=title.replace(' ','')\n",
    "count=0\n",
    "for k in sorted(kw, key=kw.get, reverse=True):\n",
    "    temp=(\"%s%g\" % (k, kw[k])).split('0')[0]\n",
    "    if 'VV'in temp:\n",
    "        continue\n",
    "    if 'VA'in temp:\n",
    "        continue\n",
    "        \n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('', temp).replace('  ',' ') # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "    result_nonspace=result.replace(' ','')\n",
    "    if len(result)<=2:\n",
    "        continue\n",
    "    if result in title:\n",
    "        continue\n",
    "    if result_nonspace in title:\n",
    "        continue\n",
    "    if result in title_nonspace:\n",
    "        continue\n",
    "    if result_nonspace in title_nonspace:\n",
    "        continue\n",
    "    if '리뷰' in result:\n",
    "        continue\n",
    "    if '소설' in result:\n",
    "        continue\n",
    "    print (result)\n",
    "    count+=1\n",
    "    if(count==5):\n",
    "        break\n",
    "    #hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    #result = hangul.sub('', temp) # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "    #print (result)\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #print(\"%s%g\" % (k, kw[k]))\n",
    "    #print(\"%s\\t%g\" % (k, kw[k]))\n",
    "    #% (k, kw[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "2\t0.09243353545450946\t 행성계 2840개에서 행성 3796개.\n",
      "0\t0.07180532982473248\t외계 행성(外界行星) 또는 계외 행성(系外行星)은 태양계 밖의 행성으로, 태양이 아닌 다른 항성 주위를 공전하고 있는 행성이다.\n",
      "5\t0.06708175840119866\t[5] 발견된 외계 행성들 중 지구와 가장 가까운 것은 프록시마 b이다.\n",
      "1\t0.06213274717329027\t 지금까지 3800여 개의 외계 행성이 발견되었으며(2018년 6월 23일 기준:\n",
      "16\t0.061672538482577546\t[8][9][10] 갈색 왜성을 도는 외계 행성들도 있으며 어떤 항성에도 속박되지 않고 우주를 떠도는 행성도 있다.\n",
      "14\t0.05461506550300031\t[6] 상대적으로 가벼운 지구질량 수 배 정도의 외계 행성들도 많이 발견되었으며 통계적 연구결과 이들 암석형 외계 행성의 수는 가스 행성보다 많은 것으로 보인다.\n",
      "10\t0.05027593582461892\t 관측 기술의 향상 덕분에 이후 외계 행성의 발견 속도는 상승했다.\n",
      "12\t0.04978326968936733\t확인된 외계 행성 대부분은 목성 또는 해왕성 정도 덩치의 가스 행성으로 추측되나 가스 행성이 외계 행성들 중 대부분을 차지한다는 의미는 아니다.\n",
      "15\t0.049776242369364135\t[7] 최근 지구와 비슷하거나 작은 질량의 행성들도 발견되고 있으며 이들 중 일부는 질량 외의 여러 속성이 지구와 비슷한 것도 있다.\n",
      "3\t0.04636504892203417\t 이 중 다중행성계는 632개) 모두 우리 은하 내에 있다.\n",
      "11\t0.04566473083044426\t 몇몇 외계 행성은 망원경으로 직접 사진을 찍었으나 대다수는 시선 속도와 같은 간접적인 방법으로 발견되었다.\n",
      "18\t0.044418000510866834\t일부 행성은 생명체 거주가능 영역 내를 돌고 있어 표면에 액체 물(또는 생명체)이 존재 가능할 것으로 보이며, 이런 행성들의 발견으로 외계 생명체의 존재 여부에 대한 관심은 증폭되고 있다.\n",
      "4\t0.044030800683019754\t[1] 우리 은하에만 수십억 개의 행성이 존재하는 것으로 추측되며[2][3][4] 대부분 항성을 돌고 있으나 일부는 홀로 우주 공간을 움직이는 떠돌이 행성이기도 하다.\n",
      "19\t0.04377354442959304\t[11] 외계 행성이 생명체를 품기에 적합한지의 폭넓은 요소들을 고려하는 것을 행성 거주 가능성 연구라고 하며 이는 외계 행성 탐사에 포함된다.\n",
      "7\t0.04130613789459777\t 19세기부터 외계 행성을 찾았다는 발표가 여러 번 있었으나 천문학자들의 검증 결과 이 모든 주장들은 기각되었다.\n",
      "8\t0.04081907590645328\t 1992년 펄서 PSR B1257+12 주위를 도는 암석 행성들의 존재가 최초로 검증, 발표되었다.\n",
      "9\t0.037496572568110946\t 주계열성을 도는 행성 중 최초로 확인된 행성은 페가수스자리 51을 4일에 한 바퀴 도는 가스 행성 페가수스자리 51 b이다.\n",
      "17\t0.03576525315788891\t 그러나 이런 특수한 상황에서 천체들에 '행성' 명칭이 항상 적용되는 것은 아니다.\n",
      "6\t0.03241526064805283\t수 세기에 걸쳐 많은 철학자와 과학자들은 외계 행성이 있으리라고 추측해 왔으나 이들이 얼마나 흔하게 있는지 또는 우리 태양계와 외계 행성계가 얼마나 비슷한지 알 방법이 없었다.\n",
      "13\t0.02836915172627931\t 단지 무거운 행성들은 쉽게 눈에 띄기 때문이며 선택 편향의 결과이다.\n",
      "외계 행성(外界行星) 또는 계외 행성(系外行星)은 태양계 밖의 행성으로, 태양이 아닌 다른 항성 주위를 공전하고 있는 행성이다.  지금까지 3800여 개의 외계 행성이 발견되었으며(2018년 6월 23일 기준:  행성계 2840개에서 행성 3796개. [5] 발견된 외계 행성들 중 지구와 가장 가까운 것은 프록시마 b이다.\n"
     ]
    }
   ],
   "source": [
    "tr = TextRank()\n",
    "print('Load...')\n",
    "from konlpy.tag import Komoran\n",
    "tagger = Komoran()\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV') ])\n",
    "tr.loadSents(RawSentenceReader('test2.txt'), lambda sent: filter(lambda x:x not in stopword and x[1] in ('NNG', 'NNP', 'VV', 'VA'), tagger.pos(sent)))\n",
    "print('Build...')\n",
    "tr.build()\n",
    "ranks = tr.rank()\n",
    "for k in sorted(ranks, key=ranks.get, reverse=True)[:100]:\n",
    "    print(\"\\t\".join([str(k), str(ranks[k]), str(tr.dictCount[k])]))\n",
    "print(tr.summarize(0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
