{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작품이름 -> \n",
      "닥터최태수\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nul=main.find(\\'ul\\')\\nli_s=ul.findAll(\\'li\\')\\nfor li in li_s:\\n    url=li.find(\\'a\\').attrs[\\'href\\'].replace(\"://\",\"://m.\")\\n    r=session.get(url,headers=header)\\n    html = bs4.BeautifulSoup(r.content, \"html.parser\")\\n    \\n    title_area=html.find(\\'div\\',{\\'class\\',\\'view_head\\'})\\n\\n    title=title_area.find(\\'h3\\',{\\'class\\',\\'tit_view\\'})\\n    print(\"제목 : \" + title.text.strip())\\n\\n    info_area=html.find(\\'div\\',{\\'class\\',\\'info_writer\\'})\\n\\n    author=info_area.find(\\'span\\',{\\'class\\',\\'txt_writer\\'})\\n    print(\"작성자 : \" + author.text.strip())\\n\\n    date=title_area.find(\\'time\\',{\\'class\\',\\'txt_time\\'})\\n    print(\"작성일시 : \" + date.text)\\n\\n    post_area=html.find(\\'div\\',{\\'class\\',\\'small\\'})\\n\\n    posts=post_area.findAll(\\'p\\')\\n    print(\"\\n----------------------------------------\\n\" + \"게시글\\n\" + \"----------------------------------------\\n\")\\n    for post in posts:\\n        print(post.text)\\n    print(\"\\n----------------------------------------\\n\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def daum_crawling(search_name) , 전체 라인 한칸씩 탭하고 input()제거할 것.\n",
    "import bs4\n",
    "import requests\n",
    "from urllib import parse\n",
    "import re\n",
    "\n",
    "session = requests.Session()\n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\"}\n",
    "\n",
    "print(\"작품이름 -> \")\n",
    "search_name=input()\n",
    "tmp = search_name + \"리뷰\"\n",
    "\n",
    "url = \"https://search.daum.net/search?w=blog&nil_search=btn&DA=STC&enc=utf8&q=\"+parse.quote(tmp) + \"&f=section&SA=daumsec&page=1\"\n",
    "#For문 사용시 마지막 count가 1씩 올라감.\n",
    "r=session.get(url,headers=header)\n",
    "c=r.content#text\n",
    "html = bs4.BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "main=html.find('div',{'class','type_fulltext wid_n'})\n",
    "\n",
    "total=html.find('span',{'class','txt_info'})#.text.split(\"/ \")[1].replace(\"건\",\"\").replace(\",\",\"\")\n",
    "print(total)\n",
    "\n",
    "ul=main.find('ul')\n",
    "li_s=ul.findAll('li')\n",
    "for li in li_s:\n",
    "    url=li.find('a').attrs['href'].replace(\"://\",\"://m.\")\n",
    "    r=session.get(url,headers=header)\n",
    "    html = bs4.BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    title_area=html.find('div',{'class','view_head'})\n",
    "\n",
    "    title=title_area.find('h3',{'class','tit_view'})\n",
    "    print(\"제목 : \" + title.text.strip())\n",
    "\n",
    "    info_area=html.find('div',{'class','info_writer'})\n",
    "\n",
    "    author=info_area.find('span',{'class','txt_writer'})\n",
    "    print(\"작성자 : \" + author.text.strip())\n",
    "\n",
    "    date=title_area.find('time',{'class','txt_time'})\n",
    "    print(\"작성일시 : \" + date.text)\n",
    "\n",
    "    post_area=html.find('div',{'class','small'})\n",
    "\n",
    "    posts=post_area.findAll('p')\n",
    "    print(\"\\n----------------------------------------\\n\" + \"게시글\\n\" + \"----------------------------------------\\n\")\n",
    "    for post in posts:\n",
    "        print(post.text)\n",
    "    print(\"\\n----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작품이름 -> \n",
      "재혼황후\n",
      "1-5\n",
      "프로그램 종료합니다\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from urllib import parse\n",
    "import re\n",
    "\n",
    "session = requests.Session()\n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\"}\n",
    "\n",
    "print(\"작품이름 -> \")\n",
    "search_name=input()\n",
    "tmp = search_name + \"리뷰\"\n",
    "\n",
    "base_url = \"https://search.daum.net/search?w=blog&nil_search=btn&DA=STC&enc=utf8&q=\"+parse.quote(tmp) + \"&f=section&SA=daumsec&page={}\"\n",
    "n=1\n",
    "pre_page=None\n",
    "while True:\n",
    "    url=base_url.format(n)\n",
    "    r=requests.get(url)\n",
    "    c=r.content\n",
    "    html = bs4.BeautifulSoup(c, \"html.parser\")\n",
    "        \n",
    "    count=html.find('span',{'class','txt_info'})\n",
    "    if(count==None):\n",
    "        print(\"프로그램 종료합니다\")\n",
    "        break\n",
    "    page=count.text.split(\" /\")[0]\n",
    "    if(pre_page==page):\n",
    "        print(\"프로그램 종료합니다\")\n",
    "        break\n",
    "    pre_page=page\n",
    "    print(page)\n",
    "    n=n+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
